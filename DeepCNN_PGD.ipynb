{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Import Libraries"
      ],
      "metadata": {
        "id": "x7COtkPJDqAf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXQcn-F8DO3w"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Downloading and Feature-Target Separation"
      ],
      "metadata": {
        "id": "Je74GogKDxNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "\n",
        "train_features = train_dataset.data.unsqueeze(1).float() / 255.0\n",
        "train_targets = train_dataset.targets\n",
        "\n",
        "test_features = test_dataset.data.unsqueeze(1).float() / 255.0\n",
        "test_targets = test_dataset.targets\n",
        "\n",
        "train_features = (train_features - 0.5) / 0.5\n",
        "test_features = (test_features - 0.5) / 0.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fyo2LjJOD1Iy",
        "outputId": "aa641b22-aeab-4914-902d-a67617716f01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:02<00:00, 4.57MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 135kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.28MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 2.84MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PGD Attack"
      ],
      "metadata": {
        "id": "X4ftVBLOEXW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Projected Gradient Descent function generates adversarial examples by iteratively perturbing the input images within a specified epsilon (eps) and updates are made according to the gradient of the loss concerning the input image."
      ],
      "metadata": {
        "id": "o3Ap5nnHElEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pgd_attack(model, images, labels, eps=0.3, alpha=0.01, iters=40):\n",
        "    model.eval()\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    ori_images = images.data\n",
        "\n",
        "    for _ in range(iters):\n",
        "        images.requires_grad = True\n",
        "        outputs = model(images)\n",
        "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        adv_images = images + alpha * images.grad.sign()\n",
        "        eta = torch.clamp(adv_images - ori_images, min=-eps, max=eps)\n",
        "        images = torch.clamp(ori_images + eta, min=0, max=1).detach_()\n",
        "\n",
        "    return images"
      ],
      "metadata": {
        "id": "owI2qHn7EbIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Deep CNN model"
      ],
      "metadata": {
        "id": "7C_N635XEn-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DeepCNN has 4 convolutional layers with ReLU activations,2 max-pooling layers for down-sampling and Fully connected layers with Dropout to prevent overfitting."
      ],
      "metadata": {
        "id": "SAN3tiONEyWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)  # Output: 28x28x64\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)  # Output: 28x28x128\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 14x14x128\n",
        "\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)  # Output: 14x14x256\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)  # Output: 14x14x512\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 7x7x512\n",
        "\n",
        "        self.fc1 = nn.Linear(7 * 7 * 512, 1024)\n",
        "        self.relu5 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.relu6 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc3 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu1(self.conv1(x))\n",
        "        x = self.relu2(self.conv2(x))\n",
        "        x = self.pool1(x)\n",
        "        x = self.relu3(self.conv3(x))\n",
        "        x = self.relu4(self.conv4(x))\n",
        "        x = self.pool2(x)\n",
        "        x = x.view(-1, 7 * 7 * 512)  # Flatten\n",
        "        x = self.dropout1(self.relu5(self.fc1(x)))\n",
        "        x = self.dropout2(self.relu6(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "2Y-9HpPDE1ZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initialize the model"
      ],
      "metadata": {
        "id": "7fcApeX3E3y3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DeepCNN().to(device)\n"
      ],
      "metadata": {
        "id": "zZDSfA3HE7PQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Clubbing perturbed and clean features"
      ],
      "metadata": {
        "id": "oWNt4fguE91Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_new_dataset(model, features, targets, eps=0.3, alpha=0.01, iters=40):\n",
        "\n",
        "    original_features = features.clone()\n",
        "    targets = targets.clone()\n",
        "\n",
        "\n",
        "    perturbed_features = []\n",
        "    batch_size = 128\n",
        "    model.eval()\n",
        "\n",
        "    for i in range(0, len(features), batch_size):\n",
        "        batch_features = features[i:i+batch_size].to(device)\n",
        "        batch_targets = targets[i:i+batch_size].to(device)\n",
        "        perturbed_batch = pgd_attack(model, batch_features, batch_targets, eps, alpha, iters)\n",
        "        perturbed_features.append(perturbed_batch.cpu())\n",
        "\n",
        "    perturbed_features = torch.cat(perturbed_features, dim=0)\n",
        "\n",
        "\n",
        "    combined_features = torch.cat([original_features, perturbed_features], dim=0)\n",
        "    combined_targets = torch.cat([targets, targets], dim=0)\n",
        "\n",
        "    return TensorDataset(combined_features, combined_targets)"
      ],
      "metadata": {
        "id": "ce8VFYVRFBZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate perturbed training and testing datasets\n",
        "new_train_dataset = generate_new_dataset(model, train_features, train_targets, eps=0.3, alpha=0.01, iters=40)\n",
        "new_test_dataset = generate_new_dataset(model, test_features, test_targets, eps=0.3, alpha=0.01, iters=40)\n",
        "\n",
        "# Create DataLoader for both new training and testing datasets\n",
        "new_train_loader = DataLoader(new_train_dataset, batch_size=64, shuffle=True)\n",
        "new_test_loader = DataLoader(new_test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "g97Fc8jXFErG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "lESA67pFFF3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train_model(model, train_loader, epochs=5):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for data, target in train_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += target.size(0)\n",
        "            correct += predicted.eq(target).sum().item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}: Loss = {epoch_loss:.4f}, Accuracy = {100. * correct / total:.2f}%\")\n",
        "\n",
        "train_model(model, new_train_loader, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUiOPfsCFITg",
        "outputId": "c4438aa3-7cbd-4d15-ae7c-38d67d44a782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss = 33.5041, Accuracy = 99.69%\n",
            "Epoch 2: Loss = 25.8241, Accuracy = 99.72%\n",
            "Epoch 3: Loss = 29.0396, Accuracy = 99.70%\n",
            "Epoch 4: Loss = 23.9841, Accuracy = 99.73%\n",
            "Epoch 5: Loss = 28.9961, Accuracy = 99.68%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation"
      ],
      "metadata": {
        "id": "wNVC0UIBGIf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            outputs = model(data)\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct += predicted.eq(target).sum().item()\n",
        "            total += target.size(0)\n",
        "\n",
        "    print(f\"Test Accuracy: {100. * correct / total:.2f}%\")\n",
        "\n",
        "test_loader = DataLoader(TensorDataset(test_features, test_targets), batch_size=64, shuffle=False)\n",
        "\n",
        "evaluate_model(model, new_test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hvr-HvhEGNPe",
        "outputId": "5dd45982-0e5b-4741-c72e-c2816189e1af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 99.41%\n"
          ]
        }
      ]
    }
  ]
}