{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Importing Libraries"
      ],
      "metadata": {
        "id": "WfyDfXbT7F_p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdOu9iT6uz9z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Downloading and Feature-Target Separation"
      ],
      "metadata": {
        "id": "0k2k1no-7iv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have downloaded the MNIST dataset  and performed Feature-Target separation by  separating the images (train_features and test_features) from their corresponding labels (train_targets and test_targets), then converted the image data to tensors. The Normalize transformation scales the pixel values to have a mean of 0.5 and a standard deviation of 0.5, to maintain balance so that activation functions like ReLU function more effectively."
      ],
      "metadata": {
        "id": "jWmG0EYO8s9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "\n",
        "train_features = train_dataset.data.unsqueeze(1).float() / 255.0\n",
        "train_targets = train_dataset.targets\n",
        "\n",
        "test_features = test_dataset.data.unsqueeze(1).float() / 255.0\n",
        "test_targets = test_dataset.targets\n",
        "\n",
        "train_features = (train_features - 0.5) / 0.5\n",
        "test_features = (test_features - 0.5) / 0.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m132d6nOu-by",
        "outputId": "1c9e6d7d-28e9-4aee-9074-8378d79e045b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:02<00:00, 4.18MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 64.9kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:06<00:00, 245kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.16MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PGD Attack"
      ],
      "metadata": {
        "id": "o5yzyDpi-lns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we implement the PGD attack, to generate adversarial images by iteratively adding small, bounded perturbations to the input images, according to the gradient of the model’s loss, to mislead the model's predictions."
      ],
      "metadata": {
        "id": "dp-k5h4D-sBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pgd_attack(model, images, labels, eps=0.3, alpha=0.01, iters=40):\n",
        "    model.eval()\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    ori_images = images.data\n",
        "\n",
        "    for _ in range(iters):\n",
        "        images.requires_grad = True\n",
        "        outputs = model(images)\n",
        "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        adv_images = images + alpha * images.grad.sign()\n",
        "        eta = torch.clamp(adv_images - ori_images, min=-eps, max=eps)\n",
        "        images = torch.clamp(ori_images + eta, min=0, max=1).detach_()\n",
        "\n",
        "    return images"
      ],
      "metadata": {
        "id": "P_HPgIwWvDud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CNN model"
      ],
      "metadata": {
        "id": "WJEoaG4iAnpx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A CNN model with 2 convolutional layers, each followed by batch normalization, ReLU activation, and max pooling to downsample feature maps. The flattened output passes through a fully connected layer and a dropout layer to reduce overfitting. The final layer outputs a probability distribution over the ten digit classes."
      ],
      "metadata": {
        "id": "-c822hoLAryO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2)  # Output: 28x28x32\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 14x14x32\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2)  # Output: 14x14x64\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 7x7x64\n",
        "\n",
        "        self.fc1 = nn.Linear(7 * 7 * 64, 1024)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(1024, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = x.view(-1, 7 * 7 * 64)  # Flatten\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "pBJ_X5dwvHcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check if  GPU is available and set the device accordingly."
      ],
      "metadata": {
        "id": "DhNXmAnmCVx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SimpleCNN().to(device)"
      ],
      "metadata": {
        "id": "XZdaQNW1vLX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Clubbing perturbed and clean features"
      ],
      "metadata": {
        "id": "iPwSVJkoDs9d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We prepare a new dataset by applying the PGD attack to the original features in batches, creating adversarial examples, and then combining the original and adversarial data with corresponding targets."
      ],
      "metadata": {
        "id": "8qcKsvKUD5s_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_new_dataset(model, features, targets, eps=0.3, alpha=0.01, iters=40):\n",
        "\n",
        "    original_features = features.clone()\n",
        "    targets = targets.clone()\n",
        "\n",
        "\n",
        "    perturbed_features = []\n",
        "    batch_size = 128\n",
        "    model.eval()\n",
        "\n",
        "    for i in range(0, len(features), batch_size):\n",
        "        batch_features = features[i:i+batch_size].to(device)\n",
        "        batch_targets = targets[i:i+batch_size].to(device)\n",
        "        perturbed_batch = pgd_attack(model, batch_features, batch_targets, eps, alpha, iters)\n",
        "        perturbed_features.append(perturbed_batch.cpu())\n",
        "\n",
        "    perturbed_features = torch.cat(perturbed_features, dim=0)\n",
        "\n",
        "\n",
        "    combined_features = torch.cat([original_features, perturbed_features], dim=0)\n",
        "    combined_targets = torch.cat([targets, targets], dim=0)\n",
        "\n",
        "    return TensorDataset(combined_features, combined_targets)"
      ],
      "metadata": {
        "id": "DVd9GSqyvOIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It creates a DataLoader with a batch size of 64 and enables data shuffling, preparing the dataset for training and testing."
      ],
      "metadata": {
        "id": "nipra6eQFp4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating perturbed training and testing datasets\n",
        "new_train_dataset = generate_new_dataset(model, train_features, train_targets, eps=0.3, alpha=0.01, iters=40)\n",
        "new_test_dataset = generate_new_dataset(model, test_features, test_targets, eps=0.3, alpha=0.01, iters=40)\n",
        "\n",
        "new_train_loader = DataLoader(new_train_dataset, batch_size=64, shuffle=True)\n",
        "new_test_loader = DataLoader(new_test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "FV25_HpjvRNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training the CNN Model on New Dataset"
      ],
      "metadata": {
        "id": "By8PMo89GUOa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we train the CNN model on the new dataset, containing adversarial examples. It uses the Adam optimizer with a learning rate of 0.001 and cross-entropy loss for classification. During each epoch, the model processes the data in batches, calculates the loss, and updates its weights using backpropagation."
      ],
      "metadata": {
        "id": "HYfsIbAEGHgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train_model(model, train_loader, epochs=5):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for data, target in train_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += target.size(0)\n",
        "            correct += predicted.eq(target).sum().item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}: Loss = {epoch_loss:.4f}, Accuracy = {100. * correct / total:.2f}%\")\n",
        "\n",
        "train_model(model, new_train_loader, epochs=5)"
      ],
      "metadata": {
        "id": "P7x6wRqXvVY5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94a5e0d8-c143-43fa-cb35-a12ec6b1ec95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss = 175.4103, Accuracy = 97.19%\n",
            "Epoch 2: Loss = 53.0943, Accuracy = 99.15%\n",
            "Epoch 3: Loss = 34.6272, Accuracy = 99.44%\n",
            "Epoch 4: Loss = 24.3543, Accuracy = 99.59%\n",
            "Epoch 5: Loss = 20.9508, Accuracy = 99.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Evaluation"
      ],
      "metadata": {
        "id": "VqxkoR2xGcAh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is evaluated on the test dataset and predictions are compared to actual labels, and the model’s accuracy on the test dataset is calculated."
      ],
      "metadata": {
        "id": "U0ErB9VdGlvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            outputs = model(data)\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct += predicted.eq(target).sum().item()\n",
        "            total += target.size(0)\n",
        "\n",
        "    print(f\"Test Accuracy: {100. * correct / total:.2f}%\")\n",
        "\n",
        "test_loader = DataLoader(TensorDataset(test_features, test_targets), batch_size=64, shuffle=False)\n",
        "\n",
        "evaluate_model(model, new_test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kicmy_ErvaAd",
        "outputId": "b11115da-218e-4039-f765-9221d40e7572"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 99.19%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Trained Model saved!!"
      ],
      "metadata": {
        "id": "BP78c2HnGtkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"handwritten_digit_model.pth\")"
      ],
      "metadata": {
        "id": "mUmUWZrOvluY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"handwritten_digit_model.pth\")"
      ],
      "metadata": {
        "id": "1Bo_tTu5voPd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "a1ceb9a5-73e3-47e8-e673-3e53b04d4e9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_758d32fe-77ef-42c4-8908-2d7555ce67d7\", \"handwritten_digit_model.pth\", 13102080)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}